{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TLVQ_LI6kSmK"},"source":["# **FINAL PROJECT**\n"]},{"cell_type":"code","metadata":{"id":"FbuxoArHhjpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623702088490,"user_tz":-120,"elapsed":16386,"user":{"displayName":"GERMAN PEDRERO","photoUrl":"","userId":"03283296285684263515"}},"outputId":"6d77c1ae-e67d-4db6-9893-efbae330b34e"},"source":["# basic libraries\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageOps\n","import scipy.io as sio\n","from google.colab import drive\n","import numpy as np\n","import gc\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/DeepLearning_2021/FINAL_PROJECT/Data/'\n","results_path = '/content/drive/My Drive/DeepLearning_2021/FINAL_PROJECT/Results/'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F2WYoxkysHGe"},"source":["# Compute model paramters\n","def compute_model_params(model):\n","  params = 0\n","  for p in model.parameters():\n","    params+= p.numel()\n","  return params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2lRxwSv5aVx"},"source":["# **DATA LOADERS**"]},{"cell_type":"markdown","metadata":{"id":"0yMv0ti50zy9"},"source":["## **Data loader - .MAT LOADER - EXECUTE ONLY CLASS DEFINITION** \n","Loads a .mat file dataset. It assumes the structure is an 'images' tag which contains a 4 dim array with the same structure as PNGFILES created --> [ image selector, width, height, RGB ]\n","\n","This class is useful as mat files load way faster than reading single png files. "]},{"cell_type":"code","metadata":{"id":"KY-lF8fQ1GVf"},"source":["#Making native class loader for raw png files in a folder \n","class MATDATASET(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = results_path + \"realImages.mat\",transform = None, maxImages = -1):\n","      self.transform = transform\n","      self.data = sio.loadmat( dataDir )['images']\n","\n","      self.channels = self.data.shape[3] if len(self.data.shape) > 3 else 1 \n","      \n","      if ( maxImages > 0 ):\n","        self.data = self.data[:maxImages]        \n","     # What to do to load a single item in the dataset ( read image )    \n","    def __getitem__(self, index):\n","      data = None\n","      if ( self.channels > 1):\n","        data = self.data[index,:,:,:]\n","      else:\n","        data = self.data[index,:,:]\n","\n","      data = Image.fromarray(data)\n","      # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","      if self.transform is not None : \n","        data = self.transform(data)\n","        \n","      # return the image and the label\n","      return data\n","      \n","      pass\n","    \n","    # Return the number of images\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eThdaBZc352e"},"source":["tr = tf.Compose([\n","        tf.ToTensor(), \n","        #tf.Normalize(mean = [.5, .5,.5], std = [.5,.5,.5])\n","        ])\n","MATReal = MATDATASET( results_path+\"realImages.mat\", tr)\n","MATPixel = MATDATASET (results_path+\"pixelArtImages.mat\",tr)\n","\n","train_loader_MatReal = torch.utils.data.DataLoader(dataset=MATReal,\n","                                               batch_size=100, \n","                                               shuffle=True)\n","train_loader_MatPixel = torch.utils.data.DataLoader(dataset=MATPixel,\n","                                               batch_size=100, \n","                                               shuffle=True)\n","\n","# See first image of first batch of data\n","images = next(iter(train_loader_MatReal))\n","print (images.shape)\n","plt.imshow(tf.ToPILImage(mode='RGB')(images[0]))\n","plt.show()\n","\n","# See first image of first batch of data\n","images = next(iter(train_loader_MatPixel))\n","print (images.shape)\n","plt.imshow(tf.ToPILImage(mode='RGB')(images[0]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhFfs22t5yiP"},"source":["def simplifyImages ( images, targetSizeImage= 32):\n","  data = None\n","  for i in range(len(images)):\n","    result = Image.fromarray(images[i]).convert(\"L\")\n","    #result = result.resize( (targetSizeImage, targetSizeImage), resample=Image.BICUBIC )\n","\n","    # insert image into own array of data\n","    if data is None:\n","      data = np.array( [ np.asarray(result) ] )\n","    else:\n","      data = np.append( data, [ np.asarray(result) ] , axis = 0 )\n","  return data\n","\n","\n","\n","tr = tf.Compose([\n","        tf.ToTensor(), \n","        #tf.Normalize(mean = [.5, .5,.5], std = [.5,.5,.5])\n","        ])\n","MATReal = MATDATASET( results_path+\"realImages.mat\", tr)\n","MATPixel = MATDATASET (results_path+\"pixelArtImages.mat\",tr)\n","\n","\n","sio.savemat(results_path + 'realImages_simplified.mat', {'images': simplifyImages( MATReal.data ) }) # to store data as a single mat file, opened with scipy.io.loadmat\n","sio.savemat(results_path + 'pixelArtImages_simplified.mat', {'images': simplifyImages( MATPixel.data ) }) # to store data as a single mat file, opened with scipy.io.loadmat\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOeedTZp-dvF"},"source":["tr = tf.Compose([\n","        tf.ToTensor(), \n","        #tf.Normalize(mean = [.5, .5,.5], std = [.5,.5,.5])\n","        ])\n","MATReal = MATDATASET( results_path+\"realImages_simplified.mat\", tr)\n","MATPixel = MATDATASET (results_path+\"pixelArtImages_simplified.mat\",tr)\n","\n","train_loader_MatReal = torch.utils.data.DataLoader(dataset=MATReal,\n","                                               batch_size=100, \n","                                               shuffle=True)\n","train_loader_MatPixel = torch.utils.data.DataLoader(dataset=MATPixel,\n","                                               batch_size=100, \n","                                               shuffle=True)\n","\n","# See first image of first batch of data\n","images = next(iter(train_loader_MatReal))\n","print (images.shape)\n","plt.imshow(tf.ToPILImage(mode='L')(images[0]), cmap='gray')\n","plt.show()\n","\n","# See first image of first batch of data\n","images = next(iter(train_loader_MatPixel))\n","print (images.shape)\n","plt.imshow(tf.ToPILImage(mode='L')(images[0]), cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvNcB1Dz4jwi"},"source":["# **GENERATOR**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZ-NZ-WInFKd","executionInfo":{"status":"ok","timestamp":1623702127318,"user_tz":-120,"elapsed":11,"user":{"displayName":"GERMAN PEDRERO","photoUrl":"","userId":"03283296285684263515"}},"outputId":"0cdc4ca9-2196-45ee-bf45-857cafd25ceb"},"source":["class Generator( nn.Module ):\n","  def __init__( self ):\n","    super( Generator , self).__init__()\n","\n","    self.INorm16 = nn.InstanceNorm2d(16) \n","    self.INorm64 = nn.InstanceNorm2d(64) \n","    self.INorm128 = nn.InstanceNorm2d(128) \n","\n","    self.activation = nn.LeakyReLU(negative_slope=0.3)\n","    self.pooling = nn.MaxPool2d(2,2)\n","    self.unpooling = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","    self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3)\n","\n","    # strided separable conv\n","    self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, groups=64)\n","    self.conv2_2 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)\n","\n","    # residual - separable convs treated as 1 unit\n","    self.conv3_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv3_2 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    self.conv3_3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv3_4 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    self.conv3_5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv3_6 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    self.conv3_7 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv3_8 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    self.conv3_9 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv3_10 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    \n","    self.conv4_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n","    self.conv4_2 = nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0)\n","\n","    self.conv5_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, groups=64)\n","    self.conv5_2 = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n","\n","  def forward(self, x):\n","    out = self.INorm64( self.activation( self.conv1(x) ) )\n","    out = self.pooling(out)\n","\n","    out = self.INorm64( self.activation( self.conv2_1(out) ) )\n","    out = self.INorm128( self.activation( self.conv2_2(out) ) )\n","    out = self.pooling(out)\n","\n","\n","    # residual\n","    out = self.INorm128( self.activation( self.conv3_1(out) ) )\n","    out = out3_2 = self.INorm128( self.activation( self.conv3_2(out) ) )\n","    \n","    out = self.INorm128( self.activation( self.conv3_3(out) ) )\n","    out = self.INorm128( self.activation( self.conv3_4(out) ) )\n","\n","    out = self.INorm128( self.activation( self.conv3_5(out + out3_2) ) )\n","    out = out3_6 = self.INorm128( self.activation( self.conv3_6(out) ) )\n","\n","    out = self.INorm128( self.activation( self.conv3_7(out ) ) )\n","    out = self.INorm128( self.activation( self.conv3_8(out) ) )\n","\n","    out = self.INorm128( self.activation( self.conv3_9(out + out3_6) ) )\n","    out = self.INorm128( self.activation( self.conv3_10(out) ) )\n","\n","    # end of residual\n","    out = self.INorm128( self.activation( self.conv4_1(out) ) )\n","    out = self.INorm64( self.activation( self.conv4_2(out) ) )\n","    out = self.unpooling(out)\n","\n","    out = self.INorm64( self.activation( self.conv5_1(out) ) )\n","    out = self.activation( self.conv5_2(out) )\n","    out = self.unpooling(out)\n","\n","    return out\n","\n","compute_model_params(Generator())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["111361"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"9f0zQrKn4p8s"},"source":["# **DISCRIMINATOR**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cTYp10qw-Sx","executionInfo":{"status":"ok","timestamp":1623702127319,"user_tz":-120,"elapsed":9,"user":{"displayName":"GERMAN PEDRERO","photoUrl":"","userId":"03283296285684263515"}},"outputId":"15898af3-ba08-4893-d585-bdfbd5637375"},"source":["class Discriminator ( nn.Module ):\n","  def __init__( self ):\n","    super( Discriminator , self).__init__()\n","\n","    self.INorm64 = nn.InstanceNorm2d(64) \n","    self.INorm128 = nn.InstanceNorm2d(128) \n","    self.INorm256 = nn.InstanceNorm2d(256) \n","\n","    self.activation = nn.LeakyReLU(negative_slope=0.1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","\n","    #state =   256x256x3\n","    self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n","    #state =   128x128x64\n","\n","    # strided separable conv\n","    self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, groups=64)\n","    self.conv2_2 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)\n","    #state =   64x64x128\n","\n","    self.conv3_1 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, groups=128)\n","    self.conv3_2 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0)\n","    #state =   32x32x128\n","\n","    self.conv4_1 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, groups=128)\n","    self.conv4_2 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n","    #state =   16x16x256\n","\n","    self.conv5_1 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, groups=256)\n","    self.conv5_2 = nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)\n","    #state =   8x8x128\n","\n","    self.fc = nn.Linear( 8*8*128, 1 )\n","\n","  def forward(self, x):\n","    out = self.INorm64( self.activation( self.conv1(x) ) )\n","\n","    out = self.INorm64( self.activation( self.conv2_1(out) ) )\n","    out = self.INorm128( self.activation( self.conv2_2(out) ) )\n","\n","    out = self.INorm128( self.activation( self.conv3_1(out) ) )\n","    out = self.INorm128( self.activation( self.conv3_2(out) ) )\n","    \n","    out = self.INorm128( self.activation( self.conv4_1(out) ) )\n","    out = self.INorm256( self.activation( self.conv4_2(out) ) )\n","    \n","    out = self.INorm256( self.activation( self.conv5_1(out) ) )\n","    out = self.INorm128( self.activation( self.conv5_2(out) ) )\n","\n","    out = out.view(out.size(0), -1)\n","    out = self.fc(out)\n","    out = self.sigmoid(out)\n","    \n","    return out\n","\n","\n","compute_model_params(Discriminator())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["107905"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"LZFVsUxb4tVM"},"source":["# **CYCLE GAN**\n"]},{"cell_type":"code","metadata":{"id":"cZRjVPj-T2RP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623702732137,"user_tz":-120,"elapsed":390,"user":{"displayName":"GERMAN PEDRERO","photoUrl":"","userId":"03283296285684263515"}},"outputId":"d869b0f5-b8e8-4536-fc0e-301aad373557"},"source":["class GAN ( nn.Module ):\n","  def __init__( self ):\n","    super( GAN , self).__init__()\n","\n","    self.Gen = Generator()\n","    self.Disc = Discriminator()\n","\n","  def generate(self,  x ):\n","    return self.Gen( x )\n","\n","  def discriminate(self, x ):\n","    return self.Disc( x )\n","\n","\n","class CycleGan ( nn.Module ):\n","  def __init__( self ):\n","    super( CycleGan , self).__init__()\n","\n","    self.GanAB = GAN(); # input A, output B\n","    self.GanBA = GAN(); # input B, output A\n","    \n","  def disc_parameters(self ):\n","    return list(self.GanAB.Disc.parameters()) + list(self.GanBA.Disc.parameters())\n","\n","  def gen_parameters(self):\n","    return list(self.GanAB.Gen.parameters()) + list(self.GanBA.Gen.parameters())\n","  \n","  def trainNN(self, train_loader_A, train_loader_B, optim_gen, optim_disc, num_epochs=10, cycle_weight=0.0001, device='cpu'):\n","    self.train()\n","    l1_loss = nn.L1Loss() # already applies mean()\n","\n","    total_step = len(train_loader_A)\n","    update_generators = False\n","\n","    for epoch in range(num_epochs):\n","\n","      gen_loss_avg = 0\n","      disc_loss_avg = 0\n","      nBatches = 0\n","\n","      iteratorA = iter(train_loader_A)\n","      iteratorB = iter(train_loader_B)\n","\n","      #update_generators = True if ( epoch % 2 == 0 ) else False #alternate batches\n","      update_generators = not update_generators\n","\n","      for i in range(len(train_loader_A)):\n","        A = next(iteratorA)\n","        A = A.to( device )\n","        B = next(iteratorB) \n","        B = B.to( device )\n","\n","        AB_generations = self.GanAB.generate(A)\n","        BA_generations = self.GanBA.generate(B)\n","        \n","        AB_reconstruction = self.GanBA.generate( AB_generations ) # reconstruct A samples\n","        BA_reconstruction = self.GanAB.generate( BA_generations ) # reconstruct B samples\n","\n","        probAB_fake = torch.clamp( self.GanAB.discriminate( AB_generations ), min=0.00001, max= 0.99999) # generated images\n","        probAB_real = torch.clamp( self.GanAB.discriminate( B ) , min=0.00001, max= 0.99999)# real images\n","\n","        probBA_fake = torch.clamp( self.GanBA.discriminate( BA_generations ), min=0.00001, max= 0.99999) # generated images\n","        probBA_real = torch.clamp( self.GanBA.discriminate( A ), min=0.00001, max= 0.99999) # real images\n","\n","        # disc loss\n","        discAB_loss = -0.5*(torch.log(probAB_real) + torch.log(1-probAB_fake)).mean()\n","        discBA_loss = -0.5*(torch.log(probBA_real) + torch.log(1-probBA_fake)).mean()\n","\n","        disc_loss = discAB_loss + discBA_loss\n","        # gen loss\n","        genAB_loss = -1 * torch.log(probAB_fake).mean()\n","        genBA_loss = -1 * torch.log(probBA_fake).mean()\n","\n","        cycle_loss = (torch.pow(AB_reconstruction - A, 2)).sum() / len(A) + (torch.pow(BA_reconstruction - B,2)).sum()/ len(B) #L2 loss\n","        #cycle_loss = (torch.abs(AB_reconstruction - A)).sum() / len(A) + (torch.abs(BA_reconstruction - B)).sum()/ len(B) # L1 loss\n","        #cycle_loss = l1_loss(AB_reconstruction, A) + l1_loss( BA_reconstruction, B) # paper mentions L1 loss as preferable. This function does not work at all\n","        gen_loss =  cycle_weight * cycle_loss + genAB_loss + genBA_loss \n","        \n","        if update_generators:\n","          optim_gen.zero_grad()\n","          gen_loss.backward()\n","          optim_gen.step()\n","        else:\n","          optim_disc.zero_grad()\n","          disc_loss.backward()\n","          optim_disc.step()         \n","\n","        \n","        \n","        gen_loss_avg += gen_loss.cpu().item()\n","        disc_loss_avg += disc_loss.cpu().item()\n","\n","        nBatches+=1\n","\n","        if (i)% 75 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f} ( cycle: {:.4f}, gen-disc:{:.4f}), Disc Loss: {:.4f}' \n","                    .format(epoch+1, num_epochs, i+1, total_step, gen_loss.cpu().item(), cycle_loss.cpu().item(), (genAB_loss.cpu().item() + genBA_loss.cpu().item()), disc_loss.cpu().item()))\n","        #gc.collect()\n","        #torch.cuda.empty_cache()\n","\n","      print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                      .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","  \n","  def evalNN ( self, data_loader, mode='AB' ): # mode = from A to B, or from B to A, RecA reconstruction of A, RecB reconstruction of B\n","    self.eval()\n","\n","    with torch.no_grad():\n","      imgs = next(iter(data_loader))\n","      imgs = imgs.to(device)\n","      # get network predictions\n","      result = None\n","      if mode == 'AB':\n","        result = self.GanAB.Gen( imgs )\n","      elif mode== 'BA' :\n","        result = self.GanBA.Gen( imgs )\n","      elif mode == 'RecA':\n","        result = self.GanBA.Gen( self.GanAB.Gen( imgs ) )\n","      elif mode == 'RecB':\n","        result = self.GanAB.Gen( self.GanBA.Gen( imgs ) )\n","\n","      \n","      for i in range(len(result)):\n","        plt.subplot(1,2,1)\n","        r = torch.clamp(result[i], min=0.0001, max=0.9999)\n","        plt.imshow(tf.ToPILImage(mode='L')( r ), cmap='gray')\n","        plt.subplot(1,2,2)\n","        plt.imshow(tf.ToPILImage(mode='L')( imgs[i] ), cmap='gray')\n","        plt.show()\n","    \n","    \n","compute_model_params(CycleGan())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["438532"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"DLDlnTV743uy"},"source":["# **TRAINING**"]},{"cell_type":"code","metadata":{"id":"O1E3ngc5n0lh"},"source":["device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n","#device = torch.device('cpu')\n","cycleGAN = CycleGan()\n","cycleGAN.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfWyD8R1jx0R"},"source":["\n","tr = tf.Compose([\n","        tf.ToTensor(), \n","        #tf.Normalize(mean = [.5, .5,.5], std = [.5,.5,.5])\n","        ])\n","MATReal = MATDATASET( results_path+\"realImages_simplified.mat\", tr)\n","MATPixel = MATDATASET (results_path+\"pixelArtImages_simplified.mat\",tr)\n","\n","train_loader_MatReal = torch.utils.data.DataLoader(dataset=MATReal,\n","                                               batch_size=7, \n","                                               shuffle=True)\n","train_loader_MatPixel = torch.utils.data.DataLoader(dataset=MATPixel,\n","                                               batch_size=7, \n","                                               shuffle=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xp7FZDUnzYZ"},"source":["num_epochs = 12\n","\n","lrD = 0.00032\n","lrG = 0.0072\n","cycle_weight = 0.00092\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = torch.optim.Adam(cycleGAN.disc_parameters(), lr=lrD, weight_decay=0.0001)\n","optimizerG = torch.optim.Adam(cycleGAN.gen_parameters(), lr=lrG, weight_decay=0.0001)\n","\n","cycleGAN.trainNN(train_loader_MatReal, train_loader_MatPixel, optimizerG, optimizerD, num_epochs=num_epochs, device=device)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wx_iW8Cy48dQ"},"source":["# **TESTING**"]},{"cell_type":"code","metadata":{"id":"W8iqRkx7bSJW"},"source":["torch.save(cycleGAN.state_dict(), results_path+\"worksABit4_2.ckpt\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JreZF8nvWd-p"},"source":["\"\"\"\n","cycleGAN = CycleGan() \n","cycleGAN.load_state_dict(torch.load(results_path+\"worksABit.ckpt\"))\n","cycleGAN.to(device)\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMUpvPF0EzCl"},"source":["\n","# pixelate real images\n","print(\" A -> B \")\n","cycleGAN.evalNN( train_loader_MatReal, 'AB')\n","print(\"--------------------------------------------------------\")\n","\n","\n","# see how well reconstructs\n","print(\" Reconstruction of A \")\n","cycleGAN.evalNN( train_loader_MatReal, 'RecA')\n","print(\"--------------------------------------------------------\")\n","\n","# convert pixelated dataset to real images\n","print(\" B -> A \")\n","cycleGAN.evalNN( train_loader_MatPixel, 'BA')\n","print(\"--------------------------------------------------------\")\n","\n","# see how well reconstructs pixelated images\n","print(\" Reconstruction B \")\n","cycleGAN.evalNN( train_loader_MatPixel, 'RecB')\n","print(\"--------------------------------------------------------\")"],"execution_count":null,"outputs":[]}]}